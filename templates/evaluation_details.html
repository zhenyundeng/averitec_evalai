<p>The AVeriTeC scoring is built on the FEVER scorer. The scoring script can be found on the <a href="https://fever.ai/dataset/AVeriTeC.html">AVeriTeC Dataset page</a>. A detailed explanation of the scoring metric can be found in this <a href="https://arxiv.org/pdf/2305.13117.pdf">paper (section 6.)</a> </p>
<!--<p>The AVeriTeC score considers the correct prediction of the verdict as well as the correct retrieval of evidence. A prediction is scored 1 if at least one complete evidence set of the gold evidence is a subset of the predicted evidence <strong>and</strong> the predicted label is correct, else 0.-->
<!--<p>The scorer will produce other diagnostic scores (F1, macro-precision, macro-recall and accuracy). These will not considered for the competition other than to rank two submissions with equal AVeriTeC Scores. </p>-->
<p>The AVeriTeC score considers the correct prediction of the verdict as well as the correct retrieval of evidence. If a claim is labelled as supported, refuted, conflicting evidence/cherrypicking, additionally the evidence will be checked against the list of annotated evidence. The label will only be considered correct if the Hungarian meteor score between the provided evidence and the annotated evidence is at least 0.2/0.25/0.3.</p>

<p>For the AVeriTeC score following changes are made to the FEVER scorer: </p>
<ul>
<li>Claims in Fact-Checking datasets are typically supported or refuted by evidence, or there is not enough evidence. We add a fourth class: conflicting evidence/cherry-picking. This covers both conflicting evidence, and technically true claims that mislead by excluding important context, i.e., the claim has both supporting and refuting evidence.
<li>Unlike in FEVER using a closed source of evidence such as Wikipedia, AVERITEC is intended for use with evidence retrieved from the open web. Since the same evidence may be found in different sources, we cannot rely on exact matching to score retrieved evidence. As such, we instead rely on approximate matching. Specifically, we use the <a href="https://onlinelibrary.wiley.com/doi/abs/10.1002/nav.3800020109">Hungarian Algorithm</a> to find an optimal matching of provided evidence to annotated evidence.
</ul>
